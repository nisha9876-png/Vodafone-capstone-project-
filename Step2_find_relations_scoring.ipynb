{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9539d530",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf40cf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration - Input the Data folder here, Mark the \"Control File\", i.e. the One that as per domain knowledge has the Composite-Key, and mention the Key in \"CNTKEY_COL\"  to be splitted and traversed through the dataset\n",
    "DATA_FOLDER      = Path(r\"C:\\Users\\shobh\\OneDrive\\Desktop\\Analytics Project\\Dataset\\test_data\")\n",
    "CONTROL_FILENAME = \"ODS_CAMA_PRD.CNTRL.0.csv\"\n",
    "CNTKEY_COL       = \"cntkey\"\n",
    "MIN_SPLIT_LEN    = 2    # minimum segment length for fallback\n",
    "MAX_SPLIT_LEN    = 4    # maximum segment length for fallback\n",
    "MIN_PARTS        = 2    # minimum number of segments per key\n",
    "MAX_PARTS        = 4    # maximum number of segments per key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8e66d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_series(s: pd.Series) -> pd.Series:\n",
    "    \"\"\"Trim whitespace, drop blanks, lowercase.\"\"\"\n",
    "    s = s.fillna(\"\").astype(str).str.strip()\n",
    "    return s[s != \"\"].str.lower()\n",
    "\n",
    "def load_control_keys(folder: Path) -> pd.Index:\n",
    "    \"\"\"Load unique CNTKEYs from the control table.\"\"\"\n",
    "    df = pd.read_csv(folder / CONTROL_FILENAME, dtype=str, low_memory=False)\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    real = cols.get(CNTKEY_COL)\n",
    "    if not real:\n",
    "        raise KeyError(f\"Column '{CNTKEY_COL}' not found in control table\")\n",
    "    return normalize_series(df[real]).unique()\n",
    "\n",
    "def build_column_metadata(folder: Path) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build metadata (min/max value lengths, unique ratio) for every CSV column\n",
    "    except the control table.\n",
    "    \"\"\"\n",
    "    records = []\n",
    "    control_stem = CONTROL_FILENAME.lower().replace(\".csv\", \"\")\n",
    "    for path in folder.glob(\"*.csv\"):\n",
    "        stem = path.stem.lower()\n",
    "        if stem == control_stem:\n",
    "            continue\n",
    "        df = pd.read_csv(path, dtype=str, low_memory=False)\n",
    "        for col in df.columns:\n",
    "            vals = normalize_series(df[col])\n",
    "            lengths = vals.str.len()\n",
    "            records.append({\n",
    "                \"table\":       stem,\n",
    "                \"column\":      col,\n",
    "                \"min_len\":     int(lengths.min()) if not lengths.empty else 0,\n",
    "                \"max_len\":     int(lengths.max()) if not lengths.empty else 0,\n",
    "                \"unique_ratio\": vals.nunique() / len(vals) if len(vals) > 0 else 0\n",
    "            })\n",
    "    return pd.DataFrame(records)\n",
    "\n",
    "def whitespace_split(key: str) -> list:\n",
    "    \"\"\"\n",
    "    If key contains spaces, split on them; else return [].\n",
    "    \"\"\"\n",
    "    parts = key.split()\n",
    "    return parts if len(parts) > 1 else []\n",
    "\n",
    "def fallback_splits(key: str) -> list:\n",
    "    \"\"\"\n",
    "    Generate all splits into MIN_PARTSâ€“MAX_PARTS segments,\n",
    "    each segment length between MIN_SPLIT_LEN and MAX_SPLIT_LEN.\n",
    "    \"\"\"\n",
    "    n = len(key)\n",
    "    results = []\n",
    "    def helper(i, path):\n",
    "        if i == n:\n",
    "            if MIN_PARTS <= len(path) <= MAX_PARTS:\n",
    "                results.append(path.copy())\n",
    "            return\n",
    "        if len(path) >= MAX_PARTS:\n",
    "            return\n",
    "        for L in range(MIN_SPLIT_LEN, MAX_SPLIT_LEN + 1):\n",
    "            j = i + L\n",
    "            if j <= n:\n",
    "                path.append(key[i:j])\n",
    "                helper(j, path)\n",
    "                path.pop()\n",
    "    helper(0, [])\n",
    "    return results\n",
    "\n",
    "\n",
    "def score_column_pairs(folder: Path, metadata: pd.DataFrame) -> pd.DataFrame:\n",
    "    # 1) Load all tables\n",
    "    tables = {}\n",
    "    control_stem = CONTROL_FILENAME.lower().replace(\".csv\", \"\")\n",
    "    for path in folder.glob(\"*.csv\"):\n",
    "        stem = path.stem.lower()\n",
    "        df = pd.read_csv(path, dtype=str, low_memory=False).apply(normalize_series)\n",
    "        tables[stem] = df\n",
    "\n",
    "    # 2) Metadata lookup\n",
    "    meta_map = {(row.table, row.column): row for row in metadata.itertuples()}\n",
    "\n",
    "    # 3) Load CNTKEYs\n",
    "    keys = load_control_keys(folder)\n",
    "\n",
    "    # 4) Accumulate scores\n",
    "    acc = defaultdict(lambda: {\"score\": 0, \"examples\": set()})\n",
    "    for key in keys:\n",
    "        parts = whitespace_split(key)\n",
    "        splits = [parts] if parts else fallback_splits(key)\n",
    "\n",
    "        for split in splits:\n",
    "            # find candidate columns for each segment\n",
    "            candidates = []\n",
    "            for seg in split:\n",
    "                L = len(seg)\n",
    "                cand = []\n",
    "                for (tbl, col), meta in meta_map.items():\n",
    "                    if meta.min_len <= L <= meta.max_len:\n",
    "                        cnt = (tables[tbl][col] == seg).sum()\n",
    "                        if cnt > 0:\n",
    "                            cand.append((tbl, col, cnt))\n",
    "                candidates.append(cand)\n",
    "\n",
    "            # score every combination\n",
    "            for combo in product(*candidates):\n",
    "                cols = tuple((t, c) for t, c, _ in combo)\n",
    "                score = sum(cnt for _, _, cnt in combo)\n",
    "                entry = acc[cols]\n",
    "                entry[\"score\"] += score\n",
    "                if len(entry[\"examples\"]) < 5:\n",
    "                    entry[\"examples\"].add(key)\n",
    "\n",
    "    # 5) Build result DataFrame\n",
    "    rows = []\n",
    "    for cols, info in acc.items():\n",
    "        rows.append({\n",
    "            \"column_pair\":   cols,\n",
    "            \"total_score\":   info[\"score\"],\n",
    "            \"example_keys\":  \", \".join(info[\"examples\"])\n",
    "        })\n",
    "    result = pd.DataFrame(rows)\n",
    "    return result.sort_values(\"total_score\", ascending=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5893acea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top candidate column-pairs:\n",
      "                                                     column_pair  total_score                                   example_keys\n",
      "((ods_cama_prd.wordm.0, WOSREA), (ods_cama_prd.wocon.0, WCSERV))       279369   06 tdp, 10 pdha3, 10 hgob1, 10 pdmv1, 06 dks\n",
      "((ods_cama_prd.woact.0, KMDSEQ), (ods_cama_prd.wocon.0, WCSERV))       263236 10 pdha3, 10 hgob1, 10 dkw, 10 pdmv1, 10 dthb1\n",
      "((ods_cama_prd.wordm.0, WOSREA), (ods_cama_prd.ratem.0, SESRCD))       233321 10 pdha3, 10 hgob1, 10 pdmv1, 10 dthb1, 06 dks\n",
      " ((ods_cama_prd.wordm.0, WOSREA), (ods_cama_prd.wordd.0, WDSRC))       222531 10 pdha3, 10 hgob1, 10 pdmv1, 10 dthb1, 06 dks\n",
      " ((ods_cama_prd.woact.0, KMDSEQ), (ods_cama_prd.wordd.0, WDSRC))       222024 10 pdha3, 10 hgob1, 10 dkw, 10 pdmv1, 10 dthb1\n",
      "((ods_cama_prd.woact.0, KMDSEQ), (ods_cama_prd.ratem.0, SESRCD))       219255 10 pdha3, 10 hgob1, 10 dkw, 10 pdmv1, 10 dthb1\n",
      " ((ods_cama_prd.woact.0, KMDLY), (ods_cama_prd.wocon.0, WCSERV))       197307 10 pdha3, 10 hgob1, 10 dkw, 10 pdmv1, 10 dthb1\n",
      "((ods_cama_prd.woact.0, KMDSEQ), (ods_cama_prd.cucon.0, COSERV))       173624 10 pdha3, 10 hgob1, 10 dkw, 10 pdmv1, 10 dthb1\n",
      "  ((ods_cama_prd.woact.0, KMDLY), (ods_cama_prd.wordd.0, WDSRC))       167074 10 pdha3, 10 hgob1, 10 dkw, 10 pdmv1, 10 dthb1\n",
      "((ods_cama_prd.wordm.0, WOSREA), (ods_cama_prd.cucon.0, COSERV))       165735 10 pdha3, 10 hgob1, 10 pdmv1, 10 dthb1, 06 dks\n",
      "\n",
      "Results saved to cntkey_split_scores2_new.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Build metadata and score splits\n",
    "    metadata = build_column_metadata(DATA_FOLDER)\n",
    "    result   = score_column_pairs(DATA_FOLDER, metadata)\n",
    "\n",
    "    # Show top candidates\n",
    "    print(\"Top candidate column-pairs:\")\n",
    "    print(result.head(10).to_string(index=False))\n",
    "\n",
    "    # Optionally save to CSV\n",
    "    out = \"cntkey_split_scores2_new.csv\"\n",
    "    result.to_csv(out, index=False)\n",
    "    print(f\"\\nResults saved to {out}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
