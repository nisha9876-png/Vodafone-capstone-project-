{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5c91c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\".*Could not infer format.*\")\n",
    "from collections import defaultdict\n",
    "from difflib import SequenceMatcher\n",
    "from collections import defaultdict\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b4d428",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Input the Data Folder (Where all the files are) and the INFO SHEET (that has all the PK/HIST,etc marked)\n",
    "DATA_FOLDER = r\"C:\\Users\\shobh\\OneDrive\\Desktop\\Analytics Project\\Dataset\\test_data\"\n",
    "excel_path = r\"C:\\Users\\shobh\\OneDrive\\Desktop\\Analytics Project\\Dataset\\information_test_data.xlsx\"\n",
    "xl = pd.ExcelFile(excel_path)\n",
    "ID_KEYWORDS = ['id', 'key', 'ref', 'dt']\n",
    "\n",
    "def ends_with_id_keyword(col_name):\n",
    "    col_name = col_name.lower()\n",
    "    return any(col_name.endswith(k) for k in ID_KEYWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30d254b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_datetime_like(series):\n",
    "    try:\n",
    "        parsed = pd.to_datetime(series.dropna(), errors='coerce')\n",
    "        non_null_ratio = parsed.notnull().sum() / len(series.dropna())\n",
    "        return non_null_ratio >= 0.9\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7689b27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_metadata_features(folder_path):\n",
    "    records = []\n",
    "\n",
    "    for file in Path(folder_path).glob(\"*.csv\"):\n",
    "        table_name = re.sub(r\"\\.0$\", \"\", file.stem.strip().lower())\n",
    "        df = pd.read_csv(file, dtype=str, low_memory=False)\n",
    "\n",
    "        df = df.applymap(lambda x: np.nan if pd.isna(x) or str(x).strip() == '' else str(x).strip())\n",
    "        df = df.apply(lambda col: col.apply(lambda x: x.lower() if isinstance(x, str) else x))\n",
    "\n",
    "\n",
    "        for i, col in enumerate(df.columns):\n",
    "            col_data = df[col]\n",
    "\n",
    "            total = len(col_data)\n",
    "            non_null_count = col_data.notnull().sum()\n",
    "            null_percent = 100 * (total - non_null_count) / total if total > 0 else 0\n",
    "            contains_nulls = null_percent > 0\n",
    "\n",
    "            col_data_nonnull = col_data.dropna()\n",
    "            unique_ratio = col_data_nonnull.nunique() / non_null_count if non_null_count > 0 else 0\n",
    "            inferred_dtype = \"string\"\n",
    "            avg_val_len = max_val_len = min_val_len = None\n",
    "\n",
    "            try:\n",
    "                as_num = pd.to_numeric(col_data_nonnull, errors=\"coerce\")\n",
    "                if as_num.notnull().all() and not as_num.empty:\n",
    "                    if (as_num % 1 == 0).all():\n",
    "                        inferred_dtype = \"integer\"\n",
    "                        lengths = as_num.astype(str).str.len()\n",
    "                    else:\n",
    "                        inferred_dtype = \"float\"\n",
    "                        lengths = as_num.astype(str).str.replace(r\"\\..*\", \"\", regex=True).str.len()\n",
    "                else:\n",
    "                    as_date = pd.to_datetime(col_data_nonnull, errors=\"coerce\")\n",
    "                    if as_date.notnull().all() and not as_date.empty:\n",
    "                        inferred_dtype = \"datetime\"\n",
    "                        lengths = None\n",
    "                    else:\n",
    "                        inferred_dtype = \"string\"\n",
    "                        lengths = col_data_nonnull.astype(str).str.len()\n",
    "            except:\n",
    "                inferred_dtype = \"string\"\n",
    "                lengths = col_data_nonnull.astype(str).str.len()\n",
    "\n",
    "            if lengths is not None and not lengths.empty:\n",
    "                avg_val_len = lengths.mean()\n",
    "                max_val_len = lengths.max()\n",
    "                min_val_len = lengths.min()\n",
    "                mode_val_len = lengths.mode().iloc[0] if not lengths.mode().empty else None\n",
    "            else:\n",
    "                mode_val_len = None\n",
    "\n",
    "            record = {\n",
    "                \"table_name\": table_name,\n",
    "                \"column_name\": col,\n",
    "                \"column_position\": i,\n",
    "                \"name_length\": len(col),\n",
    "                \"contains_keyword_id\": ends_with_id_keyword(col),\n",
    "                \"contains_nulls\": contains_nulls,\n",
    "                \"null_percent\": round(null_percent, 2),\n",
    "                \"is_unique\": unique_ratio == 1.0 if non_null_count > 0 else False,\n",
    "                \"inferred_dtype\": inferred_dtype,\n",
    "                \"avg_val_len\": avg_val_len,\n",
    "                \"min_val_len\": min_val_len,\n",
    "                \"max_val_len\": max_val_len,\n",
    "                \"mode_val_len\": mode_val_len\n",
    "            }\n",
    "\n",
    "            records.append(record)\n",
    "\n",
    "    return pd.DataFrame(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffe549c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shobh\\AppData\\Local\\Temp\\ipykernel_16104\\1452100316.py:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: np.nan if pd.isna(x) or str(x).strip() == '' else str(x).strip())\n",
      "C:\\Users\\shobh\\AppData\\Local\\Temp\\ipykernel_16104\\1452100316.py:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: np.nan if pd.isna(x) or str(x).strip() == '' else str(x).strip())\n",
      "C:\\Users\\shobh\\AppData\\Local\\Temp\\ipykernel_16104\\1452100316.py:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: np.nan if pd.isna(x) or str(x).strip() == '' else str(x).strip())\n",
      "C:\\Users\\shobh\\AppData\\Local\\Temp\\ipykernel_16104\\1452100316.py:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: np.nan if pd.isna(x) or str(x).strip() == '' else str(x).strip())\n",
      "C:\\Users\\shobh\\AppData\\Local\\Temp\\ipykernel_16104\\1452100316.py:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: np.nan if pd.isna(x) or str(x).strip() == '' else str(x).strip())\n",
      "C:\\Users\\shobh\\AppData\\Local\\Temp\\ipykernel_16104\\1452100316.py:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: np.nan if pd.isna(x) or str(x).strip() == '' else str(x).strip())\n",
      "C:\\Users\\shobh\\AppData\\Local\\Temp\\ipykernel_16104\\1452100316.py:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: np.nan if pd.isna(x) or str(x).strip() == '' else str(x).strip())\n",
      "C:\\Users\\shobh\\AppData\\Local\\Temp\\ipykernel_16104\\1452100316.py:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: np.nan if pd.isna(x) or str(x).strip() == '' else str(x).strip())\n",
      "C:\\Users\\shobh\\AppData\\Local\\Temp\\ipykernel_16104\\1452100316.py:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: np.nan if pd.isna(x) or str(x).strip() == '' else str(x).strip())\n",
      "C:\\Users\\shobh\\AppData\\Local\\Temp\\ipykernel_16104\\1452100316.py:9: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  df = df.applymap(lambda x: np.nan if pd.isna(x) or str(x).strip() == '' else str(x).strip())\n"
     ]
    }
   ],
   "source": [
    "metadata_df = extract_metadata_features(DATA_FOLDER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ca1cfe91",
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_indicators = {\"PK\", \"SKPK\", \"PK/HIST\"}\n",
    "\n",
    "pk_labels = []\n",
    "for sheet_name in xl.sheet_names:\n",
    "    df = xl.parse(sheet_name)\n",
    "    if \"Column Name\" in df.columns and \"Type\" in df.columns:\n",
    "        for _, row in df.iterrows():\n",
    "            col_name = str(row[\"Column Name\"]).strip().lower()\n",
    "            label = str(row[\"Type\"]).strip().upper() if pd.notnull(row[\"Type\"]) else \"\"\n",
    "            if label in pk_indicators:\n",
    "                pk_labels.append((sheet_name.lower(), col_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b65c7dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df[\"table_name_clean\"] = metadata_df[\"table_name\"].str.strip().str.lower().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "metadata_df[\"column_name_clean\"] = metadata_df[\"column_name\"].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d86ed906",
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_set = set(pk_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daca4fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df[\"is_primary_key\"] = metadata_df.apply(\n",
    "    lambda row: 1 if (row[\"table_name_clean\"], row[\"column_name_clean\"]) in pk_set else 0,\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff82e7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df.drop(columns=[\"table_name_clean\", \"column_name_clean\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f3895b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_lookup = defaultdict(set)\n",
    "for tbl, col in pk_labels:\n",
    "    pk_lookup[tbl].add(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f232507a",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_df[\"table_name_lower\"] = metadata_df[\"table_name\"].str.strip().str.lower().str.replace(r\"\\.0$\", \"\", regex=True)\n",
    "metadata_df[\"column_name_lower\"] = metadata_df[\"column_name\"].str.strip().str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f460d373",
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_columns_df = metadata_df[metadata_df[\"is_primary_key\"] == 1]\n",
    "fk_candidate_df = metadata_df.copy()  # include all columns as FK candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5fbff608",
   "metadata": {},
   "outputs": [],
   "source": [
    "pk_groups = defaultdict(list)\n",
    "for _, row in pk_columns_df.iterrows():\n",
    "    pk_groups[row[\"table_name_lower\"]].append(row[\"column_name_lower\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b73068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def profile_columns_by_type(folder_path, metadata_df):\n",
    "    stats_records = []\n",
    "\n",
    "    for file in tqdm(Path(folder_path).glob(\"*.csv\"), desc=\"Processing CSV files\"):\n",
    "        try:\n",
    "            df = pd.read_csv(file, dtype=str, low_memory=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not read {file.name}: {e}\")\n",
    "            continue\n",
    "\n",
    "        table_name = re.sub(r\"\\.0$\", \"\", file.stem.strip().lower())\n",
    "\n",
    "        # Use cleaned name to match\n",
    "        subset = metadata_df[metadata_df[\"table_name_lower\"] == table_name]\n",
    "\n",
    "        for _, row in tqdm(subset.iterrows(), total=len(subset), desc=f\"Profiling {table_name}\", leave=False):\n",
    "            col_name = row[\"column_name\"]\n",
    "            dtype = row[\"inferred_dtype\"]\n",
    "            stat = {\n",
    "                \"table_name\": table_name,\n",
    "                \"column_name\": col_name,\n",
    "                \"inferred_dtype\": dtype\n",
    "            }\n",
    "\n",
    "            if col_name not in df.columns:\n",
    "                stat[\"error\"] = \"missing_column\"\n",
    "                stats_records.append(stat)\n",
    "                continue\n",
    "\n",
    "            col_data = df[col_name].dropna()\n",
    "\n",
    "            try:\n",
    "                if dtype in [\"integer\", \"float\", \"floating\"]:\n",
    "                    num_data = pd.to_numeric(col_data, errors='coerce').dropna()\n",
    "                    if not num_data.empty:\n",
    "                        stat.update({\n",
    "                            \"min\": num_data.min(),\n",
    "                            \"max\": num_data.max(),\n",
    "                            \"mean\": num_data.mean(),\n",
    "                            \"median\": num_data.median(),\n",
    "                            \"std\": num_data.std(),\n",
    "                            \"num_unique\": num_data.nunique(),\n",
    "                            \"num_nonnull\": num_data.count()\n",
    "                        })\n",
    "\n",
    "                elif dtype == \"string\":\n",
    "                    str_lengths = col_data.astype(str).str.len()\n",
    "                    stat.update({\n",
    "                        \"min_len\": str_lengths.min(),\n",
    "                        \"max_len\": str_lengths.max(),\n",
    "                        \"avg_len\": str_lengths.mean(),\n",
    "                        \"num_unique\": col_data.nunique(),\n",
    "                        \"most_freq_value\": col_data.mode().iloc[0] if not col_data.mode().empty else np.nan\n",
    "                    })\n",
    "\n",
    "                elif dtype == \"datetime\":\n",
    "                    dt_parsed = pd.to_datetime(col_data, errors='coerce').dropna()\n",
    "                    if not dt_parsed.empty:\n",
    "                        stat.update({\n",
    "                            \"min_date\": dt_parsed.min(),\n",
    "                            \"max_date\": dt_parsed.max(),\n",
    "                            \"num_unique_dates\": dt_parsed.nunique()\n",
    "                        })\n",
    "            except Exception as e:\n",
    "                stat[\"error\"] = str(e)\n",
    "\n",
    "            stats_records.append(stat)\n",
    "\n",
    "    return pd.DataFrame(stats_records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a0c3bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_fk_pk_score(pk_row, fk_row, length_tolerance=0.2):\n",
    "    score = 0\n",
    "    reasons = []\n",
    "\n",
    "    if fk_row[\"null_percent\"] >= 100:\n",
    "        return 0, [\"fk_is_100%_null\"]\n",
    "    if pk_row[\"inferred_dtype\"] != fk_row[\"inferred_dtype\"]:\n",
    "        return 0, [\"dtype_mismatch\"]\n",
    "\n",
    "    name_sim = SequenceMatcher(None, pk_row[\"column_name_lower\"], fk_row[\"column_name_lower\"]).ratio()\n",
    "    if name_sim > 0.7:\n",
    "        score += 3\n",
    "        reasons.append(f\"name_match:{round(name_sim,2)}\")\n",
    "\n",
    "    score += 2\n",
    "    reasons.append(\"dtype_match\")\n",
    "\n",
    "    pk_mode = pk_row.get(\"mode_val_len\")\n",
    "    fk_mode = fk_row.get(\"mode_val_len\")\n",
    "\n",
    "    if pk_mode and fk_mode:\n",
    "        ratio = fk_mode / pk_mode if pk_mode != 0 else 0\n",
    "        if 0.8 <= ratio <= 1.2:\n",
    "            score += 2\n",
    "            reasons.append(\"val_mode_len_match\")\n",
    "\n",
    "\n",
    "    if pk_row[\"avg_val_len\"] and fk_row[\"avg_val_len\"]:\n",
    "        ratio = fk_row[\"avg_val_len\"] / pk_row[\"avg_val_len\"]\n",
    "        if 1 - length_tolerance <= ratio <= 1 + length_tolerance:\n",
    "            score += 0.5\n",
    "            reasons.append(\"avg_len_similar\")\n",
    "\n",
    "    if pk_row[\"column_position\"] < 5:\n",
    "        score += 0.5\n",
    "        reasons.append(\"pk_early\")\n",
    "    if fk_row[\"column_position\"] < 5:\n",
    "        score += 0.5\n",
    "        reasons.append(\"fk_early\")\n",
    "\n",
    "    if fk_row[\"null_percent\"] < 50:\n",
    "        score += 0.5\n",
    "        reasons.append(\"fk_has_few_nulls\")\n",
    "\n",
    "    if pk_row[\"is_unique\"]:\n",
    "        score += 0.5\n",
    "        reasons.append(\"pk_is_unique\")\n",
    "    if not fk_row[\"is_unique\"]:\n",
    "        score += 0.5\n",
    "        reasons.append(\"fk_not_unique\")\n",
    "\n",
    "    return score, reasons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebeeff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = []\n",
    "\n",
    "seen_pairs = set()\n",
    "\n",
    "for _, pk in pk_columns_df.iterrows():\n",
    "    for _, fk in fk_candidate_df.iterrows():\n",
    "        if pk[\"table_name_lower\"] == fk[\"table_name_lower\"]:\n",
    "            continue\n",
    "\n",
    "        pk_id = (pk[\"table_name_lower\"], pk[\"column_name_lower\"])\n",
    "        fk_id = (fk[\"table_name_lower\"], fk[\"column_name_lower\"])\n",
    "        pair_key = tuple(sorted([pk_id, fk_id]))\n",
    "\n",
    "        if pair_key in seen_pairs:\n",
    "            continue\n",
    "\n",
    "        score, reasons = compute_fk_pk_score(pk, fk)\n",
    "        if score >= 3:\n",
    "            matches.append({\n",
    "                \"pk_table\": pk[\"table_name\"],\n",
    "                \"pk_column\": pk[\"column_name\"],\n",
    "                \"fk_table\": fk[\"table_name\"],\n",
    "                \"fk_column\": fk[\"column_name\"],\n",
    "                \"score\": round(score, 2),\n",
    "                \"reasons\": \", \".join(reasons)\n",
    "            })\n",
    "            seen_pairs.add(pair_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecae06a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_df = pd.DataFrame(matches).sort_values(by=\"score\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2500e1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def canonicalize(val):\n",
    "    if pd.isnull(val):\n",
    "        return np.nan\n",
    "    val = str(val).lower().strip()\n",
    "    return re.sub(r'\\W+', '', val)\n",
    "\n",
    "def get_sampled_values(df, col_name, sample_size=50):\n",
    "    values = df[col_name].dropna().astype(str).map(canonicalize).unique()\n",
    "    values = sorted(values)\n",
    "    if len(values) <= sample_size:\n",
    "        return set(values)\n",
    "    third = sample_size // 3\n",
    "    return set(values[:third] + values[len(values)//2 - third//2 : len(values)//2 + third//2] + values[-third:])\n",
    "\n",
    "def load_all_tables(folder_path):\n",
    "    tables = {}\n",
    "    for file in Path(folder_path).glob(\"*.csv\"):\n",
    "        name = re.sub(r\"\\.0$\", \"\", file.stem.strip().lower())\n",
    "        try:\n",
    "            df = pd.read_csv(file, dtype=str, low_memory=False)\n",
    "            tables[name] = df.applymap(lambda x: str(x).strip().lower() if pd.notnull(x) else x)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file.name}: {e}\")\n",
    "    return tables\n",
    "\n",
    "def verify_match_with_samples(row, tables):\n",
    "    pk_table = row[\"pk_table\"].strip().lower()\n",
    "    fk_table = row[\"fk_table\"].strip().lower()\n",
    "    pk_col = row[\"pk_column\"]\n",
    "    fk_col = row[\"fk_column\"]\n",
    "\n",
    "    try:\n",
    "        pk_df = tables[pk_table]\n",
    "        fk_df = tables[fk_table]\n",
    "        if pk_col not in pk_df.columns or fk_col not in fk_df.columns:\n",
    "            return 0.0\n",
    "\n",
    "        pk_vals = get_sampled_values(pk_df, pk_col)\n",
    "        fk_vals = get_sampled_values(fk_df, fk_col)\n",
    "\n",
    "        if not fk_vals:\n",
    "            return 0.0\n",
    "\n",
    "        match = len(fk_vals & pk_vals) / len(fk_vals)\n",
    "        return round(match, 3)\n",
    "    except Exception as e:\n",
    "        print(f\"Match error {pk_table}.{pk_col} -> {fk_table}.{fk_col}: {e}\")\n",
    "        return 0.0\n",
    "\n",
    "def verify_fk_matches_with_overlap(matches_df, data_folder, min_score=5.5, sample_size=50):\n",
    "    tables = load_all_tables(data_folder)\n",
    "\n",
    "    filtered_matches = matches_df[matches_df[\"score\"] >= min_score].copy()\n",
    "\n",
    "    filtered_matches[\"verified_overlap\"] = filtered_matches.apply(\n",
    "        lambda row: verify_match_with_samples(row, tables), axis=1\n",
    "    )\n",
    "\n",
    "    filtered_matches[\"verified_label\"] = filtered_matches[\"verified_overlap\"].apply(\n",
    "        lambda x: \"verified_strong\" if x > 0.8 else (\"weak_match\" if x > 0.4 else \"unverified\")\n",
    "    )\n",
    "\n",
    "    # Optional: merge back if needed\n",
    "    enriched_df = matches_df.merge(\n",
    "        filtered_matches[[\"pk_table\", \"pk_column\", \"fk_table\", \"fk_column\", \"verified_overlap\", \"verified_label\"]],\n",
    "        on=[\"pk_table\", \"pk_column\", \"fk_table\", \"fk_column\"],\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    return enriched_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "577183c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "enriched_matches_df = verify_fk_matches_with_overlap(matches_df, DATA_FOLDER, min_score=5.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "728cbc6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = enriched_matches_df.merge(\n",
    "    matches_df[[\"pk_table\", \"pk_column\", \"fk_table\", \"fk_column\"]],\n",
    "    on=[\"pk_table\", \"pk_column\", \"fk_table\", \"fk_column\"],\n",
    "    how=\"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3019e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv('inter_table_relations_scoring.csv',index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8b1bfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
